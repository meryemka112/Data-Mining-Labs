{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRBv/q3RZ52+DIRFh6BtS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meryemka112/Data-Mining-Labs/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ne2eFxFvUKe",
        "outputId": "93c59862-f1b7-4d06-fee7-3b350a93ba7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mmh3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrF38OYNMGue",
        "outputId": "2252af55-ed27-4a43-d7bb-6158be200ddf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mmh3\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/103.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m102.4/103.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mmh3\n",
            "Successfully installed mmh3-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lw--GsQv0rp",
        "outputId": "3f7b45cc-8ad0-441d-8dd7-43dbfa6eaccf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,632 B 0%] [Co\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [344 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,822 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,411 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,520 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,963 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,855 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,161 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Fetched 35.0 MB in 5s (7,687 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Créer une session Spark\n",
        "# spark = SparkSession.builder.appName(\"Exemple\").getOrCreate()\n",
        "\n",
        "# # Charger un fichier CSV\n",
        "# df = spark.read.csv(\"metaverse_transactions_dataset.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# # Afficher le schéma et une partie des données\n",
        "# df.printSchema()\n",
        "# df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "EEyRhL8Svenz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(df)"
      ],
      "metadata": {
        "id": "5DFDoig1vekm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.select(['timestamp','amount']).show()"
      ],
      "metadata": {
        "id": "mN-Vw2hHvehi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark.stop()"
      ],
      "metadata": {
        "id": "l-TYC8wn2GqP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from pyspark.sql.functions import col\n",
        "import hashlib\n",
        "from pyspark.sql.functions import lower, regexp_replace, split, udf\n",
        "from pyspark.sql.types import ArrayType, StringType, LongType, IntegerType"
      ],
      "metadata": {
        "id": "Tead4yqX1Zao"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialiser Apache Spark"
      ],
      "metadata": {
        "id": "cTlT_PRoKlGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"Document Similarity\") \\\n",
        "    .getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "V_uIq55J1tN0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mmh3"
      ],
      "metadata": {
        "id": "HCxKUSA7L_Sb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classe Shingling**"
      ],
      "metadata": {
        "id": "kQzWPUL9UuHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Shingling():\n",
        "    def __init__(self, k=10):\n",
        "        \"\"\"\n",
        "        k: length of shingles (in words)\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "\n",
        "    def clean_text(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "        # Mettre en minuscule, supprimer caractères spéciaux, réduire espaces multiples\n",
        "        df = df.withColumn(text_col, lower(col(text_col)))\n",
        "        df = df.withColumn(text_col, regexp_replace(col(text_col), r\"[^a-z0-9\\s]\", \"\"))\n",
        "        df = df.withColumn(text_col, regexp_replace(col(text_col), r\"\\s+\", \" \"))\n",
        "        return df\n",
        "\n",
        "    def tokenize(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "        df = df.withColumn(\"tokens\", split(col(text_col), \"\\\\s+\"))\n",
        "\n",
        "        def clean_tokens(tokens):\n",
        "            # Supprimer None et chaînes vides\n",
        "            return [str(t).strip() for t in tokens if t and str(t).strip()]\n",
        "\n",
        "        clean_udf = udf(clean_tokens, ArrayType(StringType())) #udf(...) transforme clean_tokens en fonction distribuée Spark.#Spark ne peut pas directement appliquer des fcts Python, besoin d'un wrapper -> udf\n",
        "        df = df.withColumn(\"tokens\", clean_udf(col(\"tokens\")))\n",
        "        return df\n",
        "\n",
        "    def generate_shingles(self, df: DataFrame) -> DataFrame:\n",
        "        k = self.k\n",
        "\n",
        "        def shingle_function(tokens):\n",
        "            tokens = [str(t).strip() for t in tokens if t and str(t).strip()]\n",
        "            if len(tokens) < k:\n",
        "                return []\n",
        "            shingles = [\" \".join(tokens[i:i+k]) for i in range(len(tokens) - k + 1)]\n",
        "            # Supprimer shingles vides ou None\n",
        "            return [s for s in shingles if s and s.strip()]\n",
        "\n",
        "        shingle_udf = udf(shingle_function, ArrayType(StringType()))\n",
        "        df = df.withColumn(\"shingles\", shingle_udf(col(\"tokens\")))\n",
        "        return df\n",
        "\n",
        "    def hash_shingles(self, df: DataFrame) -> DataFrame:\n",
        "        def hash_list(shingles):\n",
        "            hashed = []\n",
        "            for s in shingles:\n",
        "                if s and s.strip():\n",
        "                    h = mmh3.hash64(s.strip(), signed=False)[0] #retourne entier non signé entre 0 et 2^64 -1\n",
        "                    h = h % (2**63)  # forcer des entier entre 0 et 2^63 -1 #Sinon j'ai eu un overflow pour LongType et donc des valeurs NULL de hashage (mon prob initial)\n",
        "                    #h = mmh3.hash(\"some string\", signed=False)  # 32-bit unsigned integer\n",
        "                    hashed.append(h)\n",
        "            return hashed\n",
        "\n",
        "        hash_udf = udf(hash_list, ArrayType(LongType()))   #longtype entre - 2^63 et 2^63 -1\n",
        "        df = df.withColumn(\"hashed_shingles\", hash_udf(col(\"shingles\")))\n",
        "        return df\n",
        "\n",
        "    def transform(self, df: DataFrame, text_col: str) -> DataFrame:\n",
        "        df = self.clean_text(df, text_col)\n",
        "        df = self.tokenize(df, text_col)\n",
        "        df = self.generate_shingles(df)\n",
        "        df = self.hash_shingles(df)\n",
        "        return df\n",
        "\n"
      ],
      "metadata": {
        "id": "GwqjFNnnveeh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Spark is a distributed computing engine for large-scale data processing.\"),\n",
        "    (2, \"Hadoop is a framework for distributed computing and storage.\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"text\"])\n",
        "\n",
        "shingler = Shingling(k=3)\n",
        "df_result = shingler.transform(df, \"text\")\n",
        "\n",
        "df_result.select(\"id\", \"shingles\", \"hashed_shingles\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcDQr33rpyqB",
        "outputId": "5c11dfb0-37e0-49b3-da14-bc7398d03ed8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id |shingles                                                                                                                                                                           |hashed_shingles                                                                                                                                                         |\n",
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1  |[spark is a, is a distributed, a distributed computing, distributed computing engine, computing engine for, engine for largescale, for largescale data, largescale data processing]|[6840193950337781150, 1695454882927409067, 7334696573069238721, 4557248500890507943, 7047552605748068725, 9105976751738112218, 3302031077382566442, 4702102867197059741]|\n",
            "|2  |[hadoop is a, is a framework, a framework for, framework for distributed, for distributed computing, distributed computing and, computing and storage]                             |[7797947910477440062, 7183807877390576587, 4119757479657162897, 6284036913113657643, 2663541191794909313, 1323817512128650965, 1977441478420615232]                     |\n",
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Version python bof\n",
        "# import hashlib\n",
        "\n",
        "# class Shingling:\n",
        "#     def __init__(self, k=10):\n",
        "#         \"\"\"\n",
        "#         k: length of shingles (in words)\n",
        "#         \"\"\"\n",
        "#         self.k = k\n",
        "\n",
        "#     def tokenize(self, text):\n",
        "#         \"\"\"\n",
        "#         Simple tokenization: split text into words\n",
        "#         \"\"\"\n",
        "#         return text.lower().split()\n",
        "\n",
        "#     def generate_shingles(self, tokens):\n",
        "#         \"\"\"\n",
        "#         Generate k-word shingles from token list\n",
        "#         \"\"\"\n",
        "#         shingles = []\n",
        "#         for i in range(len(tokens) - self.k + 1):\n",
        "#             shingle = ' '.join(tokens[i:i+self.k])\n",
        "#             shingles.append(shingle)\n",
        "#         return shingles\n",
        "\n",
        "#     def hash_shingle(self, shingle):\n",
        "#         \"\"\"\n",
        "#         Deterministic hash using MD5 truncated to 64 bits\n",
        "#         \"\"\"\n",
        "#         h = int(hashlib.md5(shingle.encode('utf-8')).hexdigest()[:16], 16)\n",
        "#         h = h % (2**63)  # ensure fits in 64-bit signed integer\n",
        "#         return h\n",
        "\n",
        "#     def shingle_document(self, text):\n",
        "#         \"\"\"\n",
        "#         Complete pipeline: tokenize → generate shingles → hash shingles\n",
        "#         Returns: ordered list of hashed shingles\n",
        "#         \"\"\"\n",
        "#         tokens = self.tokenize(text)\n",
        "#         shingles = self.generate_shingles(tokens)\n",
        "#         hashed_shingles = [self.hash_shingle(s) for s in shingles]\n",
        "#         return hashed_shingles\n"
      ],
      "metadata": {
        "id": "Xtg81zzJyAQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classe CompareSets**"
      ],
      "metadata": {
        "id": "rdSlp3JRVBCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CompareSets():\n",
        "  #méthode statique ici car n'accéde pas à l'attribut de l'objet, n'a besoin que de 2 arguments set1 et set 2\n",
        "  @staticmethod\n",
        "  def calculate_jaccard_similarity(set1,set2):\n",
        "    if not set1 or not set2:\n",
        "      return 0.0\n",
        "    s1 = set(set1)\n",
        "    s2 = set(set2)\n",
        "    intersection = s1.intersection(s2)\n",
        "    union = s1.union(s2)\n",
        "    jaccard_similarity = len(intersection) / len(union)\n",
        "    return jaccard_similarity\n"
      ],
      "metadata": {
        "id": "hiyA_Mi0veSb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinHashing():\n",
        "  def __init__(self,n):\n",
        "    self.n = n\n",
        "  def hash_set(self,shingle_set):\n",
        "    n = self.n\n",
        "    min_i = []\n",
        "    for i in range(n):\n",
        "      hashed = []\n",
        "      for x in shingle_set:\n",
        "        h = mmh3.hash(str(x), seed=i, signed=False)\n",
        "        hashed.append(h)\n",
        "      min_i.append(min(hashed))\n",
        "    return min_i\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hKxWdRYVcrVj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shingles_doc1 = [6840193950337781150, 1695454882927409067, 7334696573069238721]\n",
        "minhasher = MinHashing(n=5)\n",
        "signature = minhasher.hash_set(shingles_doc1)\n",
        "\n",
        "print(signature)\n",
        "# Output example: [122343123, 2332231, 99221231, 28381223, 112392391]\n"
      ],
      "metadata": {
        "id": "vCeCusducrOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2fe2aa6-6849-4d60-8163-726db54f35c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1325887520, 1238742759, 137587445, 39641790, 190228959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CompareSignatures():\n",
        "  @staticmethod\n",
        "  def compare(sig1,sig2):\n",
        "    if len(sig1) != len(sig2):\n",
        "      raise ValueError(\"Signatures must have the same length\")\n",
        "\n",
        "    matches = sum(1 for a,b in zip(sig1,sig2) if a == b)\n",
        "    return matches /len(sig1)\n"
      ],
      "metadata": {
        "id": "LRWsdNSFn2_v"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_cLAQHWn28L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g35Z7811n25D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}